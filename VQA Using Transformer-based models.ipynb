{"cells":[{"cell_type":"markdown","metadata":{"id":"0nvZdxTdZ_fH"},"source":["#  Improved  Visual Question Answering By Transformer\n","\n","**Student:** DAO Hoai Linh vs LE Thi Hoai Luong<br>\n","**Paper:** Vision Question Answering System Based on Roberta and Vit Model\n","<br>\n","**Date created:** 2023/04/12<br>\n","**Last modified:** 2023/04/14<br>\n","**Description:** Implementing VQA task using VisionTransformers + RoBERTa (Transformer Familly)"]},{"cell_type":"markdown","metadata":{"id":"ZoxvDZT0Z_fK"},"source":["## Setup"]},{"cell_type":"code","source":["! pip install timm transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8df0MCZD0EsO","executionInfo":{"status":"ok","timestamp":1716614610686,"user_tz":-420,"elapsed":4795,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}},"outputId":"6088c96d-a4f7-4aaa-ae2c-ba35256a1927"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.3)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.0+cu121)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.5.40)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n"]}]},{"cell_type":"code","execution_count":46,"metadata":{"id":"aCDxnV7iZ_fK","executionInfo":{"status":"ok","timestamp":1716614610686,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import os\n","import numpy as np\n","import pandas as pd\n","import timm\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import ViTModel, ViTImageProcessor\n","from transformers import AutoTokenizer, RobertaModel\n","from sklearn.preprocessing import LabelEncoder\n","from torch.utils.data import DataLoader"]},{"cell_type":"markdown","metadata":{"id":"WZig1wowZ_fL"},"source":["## Downloading the data"]},{"cell_type":"code","source":["import gdown\n","gdown.download(f\"https://drive.google.com/uc?export=download&id=1kc6XNqHZJg27KeBuoAoYj70_1rT92191\", \"dataset.zip\", quiet=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":128},"id":"oTM7Qn3Z0R0l","executionInfo":{"status":"ok","timestamp":1716614613478,"user_tz":-420,"elapsed":2794,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}},"outputId":"d3a381ca-b231-4e2d-f917-1fecf1c16f68"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From (original): https://drive.google.com/uc?export=download&id=1kc6XNqHZJg27KeBuoAoYj70_1rT92191\n","From (redirected): https://drive.google.com/uc?export=download&id=1kc6XNqHZJg27KeBuoAoYj70_1rT92191&confirm=t&uuid=6f9a1a59-e5c9-42c6-968e-f96c12769baa\n","To: /content/dataset.zip\n","100%|██████████| 196M/196M [00:01<00:00, 161MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["'dataset.zip'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["import zipfile\n","\n","zip_path = './dataset.zip'\n","extract_to_path = './dataset'\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_to_path)"],"metadata":{"id":"99vb56IN0671","executionInfo":{"status":"ok","timestamp":1716614636142,"user_tz":-420,"elapsed":22667,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}}},"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C1o5u7PXZ_fL"},"source":["## Loading data\n"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"sfhuOjeDZ_fM","executionInfo":{"status":"ok","timestamp":1716615064949,"user_tz":-420,"elapsed":420,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}}},"outputs":[],"source":["train_data = []\n","train_set_path = \"/content/dataset/vaq2.0.TrainImages.txt\"\n","val_data = []\n","val_set_path = '/content/dataset/vaq2.0.DevImages.txt'\n","test_data = []\n","test_set_path = '/content/dataset/vaq2.0.TestImages.txt'"]},{"cell_type":"code","source":["with open(train_set_path, \"r\") as f:\n","    lines = f.readlines()\n","    for line in lines:\n","        temp = line.strip().split('\\t')\n","        if len(temp) < 2:\n","            print(\"Skipping invalid line:\", line)\n","            continue\n","        qa = temp[1].split('?')\n","        if len(qa) < 2:\n","            print(\"Skipping invalid question-answer format:\", line)\n","            continue\n","        answer = qa[-1].strip()\n","        data_sample = {\n","            'image_path': temp[0].strip(),\n","            'question': '?'.join(qa[:-1]) + '?',\n","            'answer': answer\n","        }\n","        train_data.append(data_sample)\n","\n","with open(val_set_path, \"r\") as f:\n","    lines = f.readlines()\n","    for line in lines:\n","        temp = line.strip().split('\\t')\n","        if len(temp) < 2:\n","            print(\"Skipping invalid line:\", line)\n","            continue\n","        qa = temp[1].split('?')\n","        if len(qa) < 2:\n","            print(\"Skipping invalid question-answer format:\", line)\n","            continue\n","        answer = qa[-1].strip()\n","        data_sample = {\n","            'image_path': temp[0].strip(),\n","            'question': '?'.join(qa[:-1]) + '?',\n","            'answer': answer\n","        }\n","        val_data.append(data_sample)\n","\n","with open(test_set_path, \"r\") as f:\n","    lines = f.readlines()\n","    for line in lines:\n","        temp = line.strip().split('\\t')\n","        if len(temp) < 2:\n","            print(\"Skipping invalid line:\", line)\n","            continue\n","        qa = temp[1].split('?')\n","        if len(qa) < 2:\n","            print(\"Skipping invalid question-answer format:\", line)\n","            continue\n","        answer = qa[-1].strip()\n","        data_sample = {\n","            'image_path': temp[0].strip(),\n","            'question': '?'.join(qa[:-1]) + '?',\n","            'answer': answer\n","        }\n","        test_data.append(data_sample)"],"metadata":{"id":"ehd9ka6Q12fe","executionInfo":{"status":"ok","timestamp":1716615085747,"user_tz":-420,"elapsed":513,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}}},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":["## Dictionary mapping classes\n"],"metadata":{"id":"18myt88wFJY9"}},{"cell_type":"code","source":["classes = set([sample['answer'] for sample in train_data])\n","classes_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n","idx_to_classes = {idx: cls_name for idx, cls_name in enumerate(classes)}"],"metadata":{"id":"iJHv5a7AFJC7","executionInfo":{"status":"ok","timestamp":1716615102450,"user_tz":-420,"elapsed":667,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}}},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":["## Dataset Class\n"],"metadata":{"id":"tQto2iGSFFcO"}},{"cell_type":"code","source":["class VQADataset(Dataset):\n","    def __init__(self, data, classes_to_idx, img_feature_extractor=None, text_tokenizer=None, label_encoder=None, device=None, root_dir=None):\n","        self.data = data\n","        self.root_dir = root_dir\n","        self.classes_to_idx = classes_to_idx\n","        self.img_feature_extractor = img_feature_extractor\n","        self.text_tokenizer = text_tokenizer\n","        self.label_encoder = label_encoder\n","        self.device = device\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.root_dir, self.data[index]['image_path'])\n","        img = Image.open(img_path).convert('RGB')\n","\n","        if self.img_feature_extractor:\n","            img = self.img_feature_extractor(images=img, return_tensors=\"pt\")\n","            img = {k: v.to(self.device).squeeze(0) for k, v in img.items()}\n","\n","        question = self.data[index]['question']\n","        if self.text_tokenizer:\n","            question = self.text_tokenizer(\n","                question,\n","                padding=\"max_length\",\n","                max_length=20,\n","                truncation=True,\n","                return_tensors=\"pt\"\n","            )\n","            question = {k: v.to(self.device).squeeze(0) for k, v in question.items()}\n","\n","        label = self.data[index]['answer']\n","        if self.label_encoder:\n","            label = self.label_encoder.transform([label])[0]\n","        else:\n","            label = self.classes_to_idx[label]  #\n","        label = torch.tensor(label, dtype=torch.long).to(self.device)\n","\n","        sample = {\n","            'image': img,\n","            'question': question,\n","            'label': label\n","        }\n","\n","        return sample\n"],"metadata":{"id":"d4s20t0tE2f1","executionInfo":{"status":"ok","timestamp":1716615103041,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}}},"execution_count":83,"outputs":[]},{"cell_type":"markdown","source":["## Assign object for dataset"],"metadata":{"id":"jNvGX4aZGY3p"}},{"cell_type":"code","source":["img_feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n","text_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","root_dir = '/content/val2014-resised/'\n","# label_encoder = LabelEncoder()\n","# label_encoder.fit(classes)\n","\n","train_dataset = VQADataset(\n","    train_data,\n","    classes_to_idx=classes_to_idx,\n","    img_feature_extractor=img_feature_extractor,\n","    text_tokenizer=text_tokenizer,\n","    label_encoder=label_encoder,\n","    device=device,\n","    root_dir=root_dir\n",")\n","\n","val_dataset = VQADataset(\n","    val_data,\n","    classes_to_idx=classes_to_idx,\n","    img_feature_extractor=img_feature_extractor,\n","    text_tokenizer=text_tokenizer,\n","    label_encoder=label_encoder,\n","    device=device,\n","    root_dir=root_dir\n",")\n","\n","test_dataset = VQADataset(\n","    test_data,\n","    classes_to_idx=classes_to_idx,\n","    img_feature_extractor=img_feature_extractor,\n","    text_tokenizer=text_tokenizer,\n","    label_encoder=label_encoder,\n","    device=device,\n","    root_dir=root_dir\n",")"],"metadata":{"id":"PGDMXJz_GT5M","executionInfo":{"status":"ok","timestamp":1716615104882,"user_tz":-420,"elapsed":691,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}}},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":["#Model"],"metadata":{"id":"d6AcxGuzMYOi"}},{"cell_type":"code","source":["class TextEncoder(nn.Module):\n","    def __init__(self):\n","        super(TextEncoder, self).__init__()\n","        self.model = RobertaModel.from_pretrained(\"roberta-base\")\n","\n","    def forward(self, inputs):\n","        outputs = self.model(**inputs)\n","        return outputs.pooler_output\n","\n","class VisualEncoder(nn.Module):\n","    def __init__(self):\n","        super(VisualEncoder, self).__init__()\n","        self.model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n","\n","    def forward(self, inputs):\n","        outputs = self.model(**inputs)\n","        return outputs.pooler_output"],"metadata":{"id":"1uAlGM1tJJQY","executionInfo":{"status":"ok","timestamp":1716615106623,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["class Classifier(nn.Module):\n","    def __init__(self, input_size=768*2, hidden_size=512, n_layers=1, dropout_prob=0.2, n_classes=2):\n","        super(Classifier, self).__init__()\n","        self.lstm = nn.LSTM(\n","            input_size,\n","            hidden_size,\n","            num_layers=n_layers,\n","            batch_first=True,\n","            bidirectional=True\n","        )\n","        self.dropout = nn.Dropout(dropout_prob)\n","        self.fc1 = nn.Linear(hidden_size * 2, n_classes)\n","\n","    def forward(self, x):\n","        x, _ = self.lstm(x)\n","        x = self.dropout(x)\n","        x = self.fc1(x)\n","        return x"],"metadata":{"id":"l48etIo33MI7","executionInfo":{"status":"ok","timestamp":1716615107451,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","execution_count":87,"metadata":{"id":"QqIBFd9lZ_fM","executionInfo":{"status":"ok","timestamp":1716615109076,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}}},"outputs":[],"source":["class VQAModel(nn.Module):\n","    def __init__(self, visual_encoder, text_encoder, classifier):\n","        super(VQAModel, self).__init__()\n","        self.visual_encoder = visual_encoder\n","        self.text_encoder = text_encoder\n","        self.classifier = classifier\n","\n","    def forward(self, image, answer):\n","        text_out = self.text_encoder(answer)\n","        image_out = self.visual_encoder(image)\n","        x = torch.cat((text_out, image_out), dim=1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def freeze(self, visual=True, textual=True, clas=False):\n","        if visual:\n","            for n, p in self.visual_encoder.named_parameters():\n","                p.requires_grad = False\n","        if textual:\n","            for n, p in self.text_encoder.named_parameters():\n","                p.requires_grad = False\n","        if clas:\n","            for n, p in self.classifier.named_parameters():\n","                p.requires_grad = False\n"]},{"cell_type":"code","source":["n_classes = len(classes)\n","hidden_size = 1024\n","n_layers = 1\n","dropout_prob = 0.2\n","\n","text_encoder = TextEncoder().to(device)\n","visual_encoder = VisualEncoder().to(device)\n","classifier = Classifier(\n","    hidden_size=hidden_size,\n","    n_layers=n_layers,\n","    dropout_prob=dropout_prob,\n","    n_classes=n_classes\n",").to(device)\n","\n","model = VQAModel(\n","    visual_encoder=visual_encoder,\n","    text_encoder=text_encoder,\n","    classifier=classifier\n",").to(device)\n","model.freeze()\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p2i-JytdJm2s","executionInfo":{"status":"ok","timestamp":1716615111232,"user_tz":-420,"elapsed":1736,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}},"outputId":"7098e0a5-b74d-4c13-a984-c37916f835c8"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["VQAModel(\n","  (visual_encoder): VisualEncoder(\n","    (model): ViTModel(\n","      (embeddings): ViTEmbeddings(\n","        (patch_embeddings): ViTPatchEmbeddings(\n","          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (encoder): ViTEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x ViTLayer(\n","            (attention): ViTSdpaAttention(\n","              (attention): ViTSdpaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): ViTSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (intermediate): ViTIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): ViTOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","        )\n","      )\n","      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (pooler): ViTPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","  )\n","  (text_encoder): TextEncoder(\n","    (model): RobertaModel(\n","      (embeddings): RobertaEmbeddings(\n","        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","        (position_embeddings): Embedding(514, 768, padding_idx=1)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): RobertaEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): RobertaPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","  )\n","  (classifier): Classifier(\n","    (lstm): LSTM(1536, 1024, batch_first=True, bidirectional=True)\n","    (dropout): Dropout(p=0.2, inplace=False)\n","    (fc1): Linear(in_features=2048, out_features=2, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"markdown","source":["#Utils"],"metadata":{"id":"E_On5RkVNXpi"}},{"cell_type":"code","execution_count":89,"metadata":{"id":"HwcdCejsZ_fM","executionInfo":{"status":"ok","timestamp":1716615114819,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}},"collapsed":true},"outputs":[],"source":["def evaluate(model, dataloader, criterion):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    losses = []\n","    with torch.no_grad():\n","        for idx, inputs in enumerate(dataloader):\n","            images = inputs['image']\n","            questions = inputs['question']\n","            labels = inputs['label']\n","            outputs = model(images, questions)\n","            loss = criterion(outputs, labels)\n","            losses.append(loss.item())\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    loss = sum(losses) / len(losses)\n","    acc = correct / total\n","    return loss, acc\n"]},{"cell_type":"code","source":["def fit(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs):\n","    train_losses = []\n","    val_losses = []\n","    for epoch in range(epochs):\n","        batch_train_losses = []\n","        model.train()\n","        for idx, inputs in enumerate(train_loader):\n","            images = inputs['image']\n","            questions = inputs['question']\n","            labels = inputs['label']\n","            optimizer.zero_grad()\n","            outputs = model(images, questions)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            batch_train_losses.append(loss.item())\n","        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n","        train_losses.append(train_loss)\n","        val_loss, val_acc = evaluate(model, val_loader, criterion)\n","        val_losses.append(val_loss)\n","        print(f'EPOCH {epoch+1}: Train loss: {train_loss:.4f} Val loss: {val_loss:.4f} Val Acc: {val_acc:.4f}')\n","        scheduler.step()\n","    return train_losses, val_losses"],"metadata":{"id":"agpXqrT5JsV_","executionInfo":{"status":"ok","timestamp":1716615115881,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}}},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":["# Train and Evaluate"],"metadata":{"id":"uXGHjbOaPvAM"}},{"cell_type":"code","source":["batch_size = 32\n","\n","train_loader = DataLoader(dataset=VQADataset(data=train_data, classes_to_idx=classes_to_idx, img_feature_extractor=img_feature_extractor, text_tokenizer=text_tokenizer, label_encoder=label_encoder, device=device),\n","                          batch_size=batch_size,\n","                          shuffle=True,\n","                          num_workers=4)\n","\n","val_loader = DataLoader(dataset=VQADataset(data=val_data, classes_to_idx=classes_to_idx, img_feature_extractor=img_feature_extractor, text_tokenizer=text_tokenizer, label_encoder=label_encoder, device=device),\n","                        batch_size=batch_size,\n","                        shuffle=False,\n","                        num_workers=4)\n","\n","test_loader = DataLoader(dataset=VQADataset(data=test_data, classes_to_idx=classes_to_idx, img_feature_extractor=img_feature_extractor, text_tokenizer=text_tokenizer, label_encoder=label_encoder, device=device),\n","                         batch_size=batch_size,\n","                         shuffle=False,\n","                         num_workers=4)\n"],"metadata":{"id":"nopfIQo2Lyyt","executionInfo":{"status":"ok","timestamp":1716615116742,"user_tz":-420,"elapsed":384,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["import torch.multiprocessing as mp\n","\n","mp.set_start_method('spawn', force=True)"],"metadata":{"id":"Yt9gAmJIPJfX","executionInfo":{"status":"ok","timestamp":1716615171042,"user_tz":-420,"elapsed":4,"user":{"displayName":"Hoài Linh Đào","userId":"06427991247119097271"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cJBHRBSAZ_fN"},"outputs":[],"source":["lr = 1e-2\n","epochs = 50\n","scheduler_step_size = epochs * 0.6\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=0.1)\n","\n","train_losses, val_losses = fit(\n","    model,\n","    train_loader,\n","    val_loader,\n","    criterion,\n","    optimizer,\n","    scheduler,\n","    epochs\n",")\n","\n","val_loss, val_acc = evaluate(model, val_loader, criterion)\n","test_loss, test_acc = evaluate(model, test_loader, criterion)\n","\n","print('Evaluation on val/test dataset')\n","print('Val accuracy:', val_acc)\n","print('Test accuracy:', test_acc)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/nlp/ipynb/neural_machine_translation_with_transformer.ipynb","timestamp":1712692157889}],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}